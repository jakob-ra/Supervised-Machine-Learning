return(beta)
}
k_fold_crossval = function(y,X,k,alpha,lambda){
# Function for k-fold crossvalidation, returns RMSE for given alpha and lambda
n = length(y) # Number of observations
p = (dim(X)[2]) # Number of parameters
# Reshuffle data and split into k groups of equal size
reshuffled_indices = sample(seq(1,n,1), n, replace=FALSE) # Shuffle indices of our n observations
n_test = floor(n/k) # n=77 and k=10 would give 7 obs. in the test set the rest of the observations
# in the training set. For n=77, using k=11 or k=7 is advisable to get evenly sized groups.
MSE=array(0,k) # Vector to hold the MSEs for each fold
# Loop over the k folds and save MSEs
for (i in seq(1,k,1)){
# Divide data into test and training
y_test = y[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
y_train = y[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
X_test = X[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
X_train = X[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
beta = elastic_net_MM(y_train,X_train,alpha,lambda) # get beta estimates for training data from MM algorithm
fitted_val = (X_test)%*%beta # get y_hat for test data
MSE[i]=1/n_test*t(y_test-fitted_val)%*%(y_test-fitted_val) # save MSE values for test data
}
RMSE = sqrt(sum(MSE)/k)
return(RMSE)
}
min_RMSE = function(y,X,k=7,alpha_values=seq(0,1,length.out=3),lambda_values=10^seq(-3, 4, length.out = 10)){
# Tunes hyperparameters alpha and lambda via k-fold crossvalidation. Returns optimal combination of alpha and lambda
# as well as the resulting RMSE.
RMSE = matrix(0,length(alpha_values),length(lambda_values)) # create matrix to hold RMSE for each hyperparamter combination
# Loop over hyperparameter combinations
for (i in 1:length(alpha_values)){
for (j in 1:length(lambda_values)){
RMSE[i,j] = k_fold_crossval(y,X,k,alpha_values[i],lambda_values[j]) # Fill the matrix with RMSE
}
}
min_index = arrayInd(which.min(RMSE), dim(RMSE)) # Find index of lowest RMSE
alpha_min = alpha_values[min_index[1]] # alpha value at lowest RMSE
lambda_min = lambda_values[min_index[2]] # lambda value at lowest RMSE
RMSE_min = RMSE[min_index] # lowest RMSE
return(list(alpha_min,lambda_min,RMSE_min))
}
## We want to compare our result with glmnet function, which cannot cross-validate alpha -> compare for fixed alpha
alpha = 0.5
# Find best cross validated lambda from glmnet, letting the function choose the sequence of lambdas to try
result.cv <- cv.glmnet(X, y, alpha=alpha, folds=7)
print(result.cv$lambda.min) # Note that the result here varies quite a bit but is frequently equal or close to 0.05
# Find best cross validated lambda from our own function, with a fine sequence of lambdas between 0 and 1
print(min_RMSE(y,X,alpha_values=alpha,lambda_values=seq(0.01,1,0.01))[[2]]) # Results are equal or very close
## Knowing that everything works as intended, find the best combination of alpha and lambda
result = min_RMSE(y,X,alpha_values=seq(0,1,0.1),lambda_values=seq(0.01,1,0.01)) # This takes a while
print(result)
# Get point estimates on full sample for the optimal values of alpha and lambda
beta_hat = elastic_net_MM(y,X,0.6,0.04)
# set very low point estimates (rounding errors) to 0
zero_ind = arrayInd(which(beta_hat < 10^(-5)), dim(beta_hat))
beta_hat[zero_ind] = 0
print(beta_hat)
# Copy code chunks that create table output in here
if (!require("pander")){install.packages("pander");library("pander")}
# Copy code chunks that create table output in here
if (!require("pander")){install.packages("pander");library("pander")}
# *------------------------------------------------------------------
# | PROGRAM NAME: Supervised Machine Learning - HW 2
# | DATE: 16-1-2020
# | CREATED BY: Jakob Rauch
# *----------------------------------------------------------------
# Clear cache
rm(list = ls())
# Load supermarket data from github
githubURL = "https://github.com/jakob-ra/Supervised-Machine-Learning/raw/master/HW_2/supermarket1996.RData"
load(url(githubURL))
df = subset(supermarket1996, select = -c(STORE, CITY, ZIP, GROCCOUP_sum, SHPINDX) )
attach(df)
# create summary statistics
summary(supermarket1996)
# Create vector y (turnover)
y = GROCERY_sum
# Create matrix X of predictor variables
X = subset(df, select = -GROCERY_sum)
# Standardize variables
X = scale(X)
y = scale(y)
# Loss function for MM-algorithm
loss = function(y,X,alpha,lambda,beta,epsilon){
# Loss function of MM-algorithm given the data, parameters beta, hyperparameters lambda and alpha,
# and convergence threshold epsilon. Returns both the value of the loss function and the D matrix
n = length(y) # Number of observations
p = length(beta) # Number of parameters
D = matrix(0,p,p) # Initialize p times p matrix of 0s
beta_abs_list = rep(0,p) # Vector to hold absolute values of beta (will need them later)
for (j in seq(0,p,1)){
beta_abs_list[j] = abs(beta[j])
D[j,j] = 1/max(c(beta_abs_list[j],epsilon))
}
c = 1/(2*n)*t(y)%*%y + 1/2*lambda*alpha*sum(beta_abs_list)
l = 1/2*t(beta)%*%(1/n*t(X)%*%X + lambda*(1-alpha)*diag(p) + lambda*alpha*D)%*%beta - 1/n*t(beta)%*%t(X)%*%y + c
return(list(l,D))
}
# MM-algorithm for minimizing the elastic net loss function
elastic_net_MM = function(y,X,alpha,lambda,beta_0=rep(0,(dim(X)[2])),epsilon=10^(-8)){
# Fits an elastic net model via MM-algorithm. Returns vector beta_hat for given data,
# hyperparameters lambda and alpha, intitial parameter guess beta_0 (default is a vector of 0s)
# and convergence threshold epsilon (default is 10^(-8)).
n = length(y) # Number of observations
p = (dim(X)[2]) # Number of parameters
# Set values for the first iteration
beta = beta_0
k = 1
# Set l_new and l_old so that (l_old-l_new)/l_old > epsilon is true in the first iteration
l_new = 0
l_old = 1
# Iterate the approximation until convergence
while ((l_old-l_new)/l_old > epsilon){
l_and_D = loss(y,X,alpha,lambda,beta,epsilon) # Loss returns both the value of the loss function and D
l_old = l_and_D[[1]]  # Gets value l of loss function
D = l_and_D[[2]] # Gets matrix D, which we already computed in the loss function
A = 1/n*t(X)%*%X + lambda*(1-alpha)*diag(p) + lambda*alpha*D
beta = solve(A, 1/n*t(X)%*%y)
l_new = loss(y,X,alpha,lambda,beta,epsilon)[[1]]
# print(k)
k = k + 1
#print(l_old-l_new)
}
return(beta)
}
k_fold_crossval = function(y,X,k,alpha,lambda){
# Function for k-fold crossvalidation, returns RMSE for given alpha and lambda
n = length(y) # Number of observations
p = (dim(X)[2]) # Number of parameters
# Reshuffle data and split into k groups of equal size
reshuffled_indices = sample(seq(1,n,1), n, replace=FALSE) # Shuffle indices of our n observations
n_test = floor(n/k) # n=77 and k=10 would give 7 obs. in the test set the rest of the observations
# in the training set. For n=77, using k=11 or k=7 is advisable to get evenly sized groups.
MSE=array(0,k) # Vector to hold the MSEs for each fold
# Loop over the k folds and save MSEs
for (i in seq(1,k,1)){
# Divide data into test and training
y_test = y[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
y_train = y[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
X_test = X[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
X_train = X[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
beta = elastic_net_MM(y_train,X_train,alpha,lambda) # get beta estimates for training data from MM algorithm
fitted_val = (X_test)%*%beta # get y_hat for test data
MSE[i]=1/n_test*t(y_test-fitted_val)%*%(y_test-fitted_val) # save MSE values for test data
}
RMSE = sqrt(sum(MSE)/k)
return(RMSE)
}
min_RMSE = function(y,X,k=7,alpha_values=seq(0,1,length.out=3),lambda_values=10^seq(-3, 4, length.out = 10)){
# Tunes hyperparameters alpha and lambda via k-fold crossvalidation. Returns optimal combination of alpha and lambda
# as well as the resulting RMSE.
RMSE = matrix(0,length(alpha_values),length(lambda_values)) # create matrix to hold RMSE for each hyperparamter combination
# Loop over hyperparameter combinations
for (i in 1:length(alpha_values)){
for (j in 1:length(lambda_values)){
RMSE[i,j] = k_fold_crossval(y,X,k,alpha_values[i],lambda_values[j]) # Fill the matrix with RMSE
}
}
min_index = arrayInd(which.min(RMSE), dim(RMSE)) # Find index of lowest RMSE
alpha_min = alpha_values[min_index[1]] # alpha value at lowest RMSE
lambda_min = lambda_values[min_index[2]] # lambda value at lowest RMSE
RMSE_min = RMSE[min_index] # lowest RMSE
return(list(alpha_min,lambda_min,RMSE_min))
}
## We want to compare our result with glmnet function, which cannot cross-validate alpha -> compare for fixed alpha
alpha = 0.5
# Find best cross validated lambda from glmnet, letting the function choose the sequence of lambdas to try
result.cv <- cv.glmnet(X, y, alpha=alpha, folds=7)
print(result.cv$lambda.min) # Note that the result here varies quite a bit but is frequently equal or close to 0.05
# Find best cross validated lambda from our own function, with a fine sequence of lambdas between 0 and 1
print(min_RMSE(y,X,alpha_values=alpha,lambda_values=seq(0.01,1,0.01))[[2]]) # Results are equal or very close
## Knowing that everything works as intended, find the best combination of alpha and lambda
result = min_RMSE(y,X,alpha_values=seq(0,1,0.1),lambda_values=seq(0.01,1,0.01)) # This takes a while
print(result)
# Get point estimates on full sample for the optimal values of alpha and lambda
beta_hat = elastic_net_MM(y,X,0.6,0.04)
# set very low point estimates (rounding errors) to 0
zero_ind = arrayInd(which(beta_hat < 10^(-5)), dim(beta_hat))
beta_hat[zero_ind] = 0
print(beta_hat)
??cv.glmnet
if (!require("glmnet")){install.packages("glmnet");library("glmnet")}
if (!require("glmnet")){install.packages("glmnet");library("glmnet")}
# *------------------------------------------------------------------
# | PROGRAM NAME: Supervised Machine Learning - HW 2
# | DATE: 16-1-2020
# | CREATED BY: Jakob Rauch
# *----------------------------------------------------------------
# Clear cache
rm(list = ls())
# Load supermarket data from github
githubURL = "https://github.com/jakob-ra/Supervised-Machine-Learning/raw/master/HW_2/supermarket1996.RData"
load(url(githubURL))
df = subset(supermarket1996, select = -c(STORE, CITY, ZIP, GROCCOUP_sum, SHPINDX) )
attach(df)
# create summary statistics
summary(supermarket1996)
# Create vector y (turnover)
y = GROCERY_sum
# Create matrix X of predictor variables
X = subset(df, select = -GROCERY_sum)
# Standardize variables
X = scale(X)
y = scale(y)
# Loss function for MM-algorithm
loss = function(y,X,alpha,lambda,beta,epsilon){
# Loss function of MM-algorithm given the data, parameters beta, hyperparameters lambda and alpha,
# and convergence threshold epsilon. Returns both the value of the loss function and the D matrix
n = length(y) # Number of observations
p = length(beta) # Number of parameters
D = matrix(0,p,p) # Initialize p times p matrix of 0s
beta_abs_list = rep(0,p) # Vector to hold absolute values of beta (will need them later)
for (j in seq(0,p,1)){
beta_abs_list[j] = abs(beta[j])
D[j,j] = 1/max(c(beta_abs_list[j],epsilon))
}
c = 1/(2*n)*t(y)%*%y + 1/2*lambda*alpha*sum(beta_abs_list)
l = 1/2*t(beta)%*%(1/n*t(X)%*%X + lambda*(1-alpha)*diag(p) + lambda*alpha*D)%*%beta - 1/n*t(beta)%*%t(X)%*%y + c
return(list(l,D))
}
# MM-algorithm for minimizing the elastic net loss function
elastic_net_MM = function(y,X,alpha,lambda,beta_0=rep(0,(dim(X)[2])),epsilon=10^(-8)){
# Fits an elastic net model via MM-algorithm. Returns vector beta_hat for given data,
# hyperparameters lambda and alpha, intitial parameter guess beta_0 (default is a vector of 0s)
# and convergence threshold epsilon (default is 10^(-8)).
n = length(y) # Number of observations
p = (dim(X)[2]) # Number of parameters
# Set values for the first iteration
beta = beta_0
k = 1
# Set l_new and l_old so that (l_old-l_new)/l_old > epsilon is true in the first iteration
l_new = 0
l_old = 1
# Iterate the approximation until convergence
while ((l_old-l_new)/l_old > epsilon){
l_and_D = loss(y,X,alpha,lambda,beta,epsilon) # Loss returns both the value of the loss function and D
l_old = l_and_D[[1]]  # Gets value l of loss function
D = l_and_D[[2]] # Gets matrix D, which we already computed in the loss function
A = 1/n*t(X)%*%X + lambda*(1-alpha)*diag(p) + lambda*alpha*D
beta = solve(A, 1/n*t(X)%*%y)
l_new = loss(y,X,alpha,lambda,beta,epsilon)[[1]]
# print(k)
k = k + 1
#print(l_old-l_new)
}
return(beta)
}
k_fold_crossval = function(y,X,k,alpha,lambda){
# Function for k-fold crossvalidation, returns RMSE for given alpha and lambda
n = length(y) # Number of observations
p = (dim(X)[2]) # Number of parameters
# Reshuffle data and split into k groups of equal size
reshuffled_indices = sample(seq(1,n,1), n, replace=FALSE) # Shuffle indices of our n observations
n_test = floor(n/k) # n=77 and k=10 would give 7 obs. in the test set the rest of the observations
# in the training set. For n=77, using k=11 or k=7 is advisable to get evenly sized groups.
MSE=array(0,k) # Vector to hold the MSEs for each fold
# Loop over the k folds and save MSEs
for (i in seq(1,k,1)){
# Divide data into test and training
y_test = y[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
y_train = y[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
X_test = X[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
X_train = X[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
beta = elastic_net_MM(y_train,X_train,alpha,lambda) # get beta estimates for training data from MM algorithm
fitted_val = (X_test)%*%beta # get y_hat for test data
MSE[i]=1/n_test*t(y_test-fitted_val)%*%(y_test-fitted_val) # save MSE values for test data
}
RMSE = sqrt(sum(MSE)/k)
return(RMSE)
}
min_RMSE = function(y,X,k=7,alpha_values=seq(0,1,length.out=3),lambda_values=10^seq(-3, 4, length.out = 10)){
# Tunes hyperparameters alpha and lambda via k-fold crossvalidation. Returns optimal combination of alpha and lambda
# as well as the resulting RMSE.
RMSE = matrix(0,length(alpha_values),length(lambda_values)) # create matrix to hold RMSE for each hyperparamter combination
# Loop over hyperparameter combinations
for (i in 1:length(alpha_values)){
for (j in 1:length(lambda_values)){
RMSE[i,j] = k_fold_crossval(y,X,k,alpha_values[i],lambda_values[j]) # Fill the matrix with RMSE
}
}
min_index = arrayInd(which.min(RMSE), dim(RMSE)) # Find index of lowest RMSE
alpha_min = alpha_values[min_index[1]] # alpha value at lowest RMSE
lambda_min = lambda_values[min_index[2]] # lambda value at lowest RMSE
RMSE_min = RMSE[min_index] # lowest RMSE
return(list(alpha_min,lambda_min,RMSE_min))
}
## We want to compare our result with glmnet function, which cannot cross-validate alpha -> compare for fixed alpha
alpha = 0.5
# Find best cross validated lambda from glmnet, letting the function choose the sequence of lambdas to try
result.cv <- cv.glmnet(X, y, alpha=alpha, folds=7)
print(result.cv$lambda.min) # Note that the result here varies quite a bit but is frequently equal or close to 0.05
# Find best cross validated lambda from our own function, with a fine sequence of lambdas between 0 and 1
print(min_RMSE(y,X,alpha_values=alpha,lambda_values=seq(0.01,1,0.01))[[2]]) # Results are equal or very close
## Knowing that everything works as intended, find the best combination of alpha and lambda
result = min_RMSE(y,X,alpha_values=seq(0,1,0.1),lambda_values=seq(0.01,1,0.01)) # This takes a while
print(result)
# Get point estimates on full sample for the optimal values of alpha and lambda
beta_hat = elastic_net_MM(y,X,0.6,0.04)
# set very low point estimates (rounding errors) to 0
zero_ind = arrayInd(which(beta_hat < 10^(-5)), dim(beta_hat))
beta_hat[zero_ind] = 0
print(beta_hat)
seq(1,10,1)
sample(seq(1,10,1),10,replace=FALSE)
sample(seq(1,10,1),10,replace=TRUE)
floor(2/5)
*floor()
?floor()
ceiling(5/2)
ceiling(1/2)
?scale.back.lm
??scale.back.lm
View(X)
beta_hat
result.cv
View(y)
View(supermarket1996)
## We want to compare our result with glmnet function, which cannot cross-validate alpha
## -> compare for fixed alpha
alpha = 0.5
# Find best cross validated lambda from glmnet, letting the function choose
# the sequence of lambdas to try
result.cv <- cv.glmnet(X, y, alpha=alpha, folds=7)
print(result.cv$lambda.min)
# Note that the result here varies quite a bit but is frequently equal or close to 0.05
# Find best cross validated lambda from our own function,
# with a fine sequence of lambdas between 0 and 1
print(min_RMSE(y,X,alpha_values=alpha,lambda_values=seq(0.01,1,0.01))[[2]])
# --> Results are equal or very close
## Knowing that everything works as intended, find the best combination of alpha and lambda
result = min_RMSE(y,X,alpha_values=seq(0,1,0.1),lambda_values=seq(0.01,1,0.01)) # This takes a while
print(result)
install.packages("dsmle")
?install.packages
install.packages("~/Documents/Studier/TI/Supervised Machine Learning/Week 3/dsmle_1.0-4.tar.gz", repos = NULL, type = "source")
install.packages("~/Documents/Studier/TI/Supervised Machine Learning/Week 3/dsmle_1.0-4.tar.gz", repos = NULL, type = "source")
install.packages("caret")
install.packages("plotrix")
install.packages("SVMMaj")
install.packages("ISLR")
install.packages("e1071")
install.packages("~/Documents/Studier/TI/Supervised Machine Learning/Week 3/dsmle_1.0-4.tar.gz", repos = NULL, type = "source")
library(dsmle)
## Week 3 lecture code
## Initialisation of packages and data
if (!require("MASS")) install.packages("MASS")
## Week 3 lecture code
## Initialisation of packages and data
if (!require("MASS")) install.packages("MASS")
## Week 3 lecture code
## Initialisation of packages and data
if (!require("MASS")) install.packages("MASS")
if (!require("plotrix")) install.packages("plotrix")
if (!require("glmnet")) install.packages("glmnet")
if (!require("splines2")) install.packages("splines2")
if (!require("rgl")) install.packages("rgl")
if (!require("plotrix")) install.packages("plotrix")
if (!require("dsmle")) install.packages("dsmle_1.0-4.tar.gz",
repos = NULL, type = "source")
## Interaction effects
load("Advertising.Rdata")  # Load the Advertsing data set
# head() shows the first 6 rows of a matrix
# model.matrix() constructs the design matrix from a formula
head(model.matrix( ~ TV + Radio + TV*Radio, data = Advertising))
## Fit model with interaction
result <- lm(Sales ~ TV + Radio + TV*Radio, data = Advertising)
summary(result)
# The I() below means: include a new variable consisting of this function of the variable
head(model.matrix( ~ horsepower + I(horsepower^2), Auto))
result <- lm(mpg ~ horsepower + I(horsepower^2), Auto) # Fit polynomial regression of degree 2
summary(result)
x <- Auto[, "horsepower"]
y <- Auto[, "mpg"]
result1 <- lm(mpg ~ horsepower, Auto)
result2 <- lm(mpg ~ horsepower + I(horsepower^2), Auto)
result5 <- lm(mpg ~ poly(horsepower, 5), Auto)
yhat.1  <- result1$fitted.values
idx     <- order(Auto$horsepower)       # We need to reorder horsepower monotone increasing
yhat.2  <- result2$fitted.values
yhat.5  <- result5$fitted.values
plot(x, y, col = "grey",                # Make color of points grey
xlab = "Horsepower", ylab = "MPG", # Labels of x-axis and y-axis
las = 1)                           # Make vertical axis tick labels horizontal
# Add lines
lines(x, yhat.1,                        # Add the predicted line for simple regression
col = "orange", lwd = 2)          # Color of line is orange with line width 2 points
lines(x[idx], yhat.2[idx],              # Add the predicted line for quadratic regression
col = "blue", lwd = 2)            # Color of line is blue, line width is 2 points
lines(x[idx], yhat.5[idx],              # Add prediction line for pol. regr. of degree 5
col = "green", lwd = 2)           # Color of line is green, line width is 2 points
legend("topright",            # The position of the legend in the plot
legend = c("Linear", "Quadratic","Degree 5"), # Text vector of labels
col = c("orange", "blue", "green"),           # Colors of the lines
lwd = c(2, 2, 2))                             # Line widths of the lines
# Make factor edu
price.ex <-factor(c("high", "high", "high", "low", "low",  "medium"))
model.matrix(~ 1 + price.ex)
load("Credit.RData")
# ANOVA:  testing for Gender difference on Balance.
result <- aov(Balance ~ Gender, Credit)
summary(result)
coef(result)
# The same but now through lm()
result <- lm(Balance ~ Gender, Credit)
summary(result)
coef(result)
## ANOVA (= multiple regression with categorical predictors)
result <- aov(Balance ~ Student + Ethnicity + Ethnicity:Student, Credit)
summary(result)
coef(result)
## Two-way Interaction Plot
interaction.plot(Credit$Ethnicity, Credit$Student, Credit$Balance, type = "b", col = c(1:3),
leg.bty = "o",  lwd = 2, pch = c(18, 24, 22),
xlab = "Ethnicity",  ylab = "Balance", main = "Interaction Plot",
trace.label = "Student")
## Comparing Linear and iSpline models
load("Advertising.RData")
y.resp <- y <- as.vector(Advertising$Sales)     # y variable
X <- model.matrix(Sales ~ TV + Radio, data = Advertising)  # Predictor variables (as a matrix, not dataframe)
X <- scale(X[, 2:3])                            # Make columns z-scores
lin.cv <- cv.glmnet(X, y, alpha = 0, lambda = 10^seq(-2, 6, length.out = 50),
standardize = FALSE)     # Ridge regression (alpha must be 0 for ridge)
lin.cv$cvm  <- lin.cv$cvm^0.5; lin.cv$cvup <- lin.cv$cvup^0.5; lin.cv$cvlo <- lin.cv$cvlo^0.5
## Set up iSpline with interior knots of based on deciles
nknots <- 19; degree <- 5
knots <- apply(X, 2, FUN = function(x)
quantile(x, seq(1/(nknots + 1), 1 - 1/(nknots + 1), by = 1/(nknots + 1))) )
X.iSpline.list <- list()
X.iSpline.list[[1]] <- X.iSpline <- iSpline(X[, 1], knots = knots[, 1], degree = degree - 1)
for (j in 2:ncol(X)){
X.iSpline.list[[j]] <- iSpline(X[, j], knots = knots[, j], degree = degree - 1)
X.iSpline <- cbind(X.iSpline, X.iSpline.list[[j]])
}
spl.cv <- cv.glmnet(X.iSpline, y, alpha = 0, lambda = 10^seq(-2, 6, length.out = 50),
standardize = FALSE)           # Ridge regression (alpha must be 0 for ridge)
spl.cv$cvm  <- spl.cv$cvm^0.5; spl.cv$cvup <- spl.cv$cvup^0.5; spl.cv$cvlo <- spl.cv$cvlo^0.5
## Plot RMSE for Linear and iSpline models
op <- par(mfrow = c(1, 2))
plot(lin.cv, ylab = "Root Mean-Squared Error", ylim = c(0, 6), las = 1, main = "RMSE Linear Model")
plot(spl.cv, ylab = "Root Mean-Squared Error", ylim = c(0, 6), las = 1, main = "RMSE iSpline Model")
par(op)
## Show regression surface for Linear and iSpline models
options(rgl.useNULL = TRUE, rgl.printRglwidget = TRUE)
source("plot.surface.R")
plot.surface.init()
mfrow3d(1, 2, sharedMouse = TRUE)
plot.surface(coef(lin.cv, s = "lambda.min"), X, y.resp)
next3d()
plot.surface(coef(spl.cv, s = "lambda.min"), X, y.resp, X.iSpline.list)
mfrow3d(1, 1)
## Fit RBF KRR model through dsmle package
ker.cv <- cv.krr(y.resp, X, kernel.type = "nonhompolynom")
# Plot RMSE for Linear and RBF KRR models
op <- par(mfrow = c(1, 2))
plot(lin.cv, ylab = "Root Mean-Squared Error", ylim = c(0, 6), las = 1, main = "RMSE Linear Model")
plot(ker.cv, ylim = c(0, 6))
par(op)
load("/Users/Maja/Documents/Studier/TI/Supervised Machine Learning/Week 3/Airline.RData")
View(Airline)
?dummy_columns()
dummy_columns()
dummy_cols()
? dummy_cols().
?? dummy_cols().
??dummy_cols()
library(caret)
?dummyVars
------------------------------------------------------------------------
# Replace the all the variables in X by their n × k kernel basis Φ(X)
# equivalent of matrix XX⊤ becomes the n × n kernel matrix K = ΦΦ⊤ with elements kii ′ = φ⊤i φi ′
# k_ij can be directly computed from xi and xi'
#  kii′ = e−γ∥xi −xi′ ∥2 for some γ > 0 (fixe)
n <- length(Airline$1)
------------------------------------------------------------------------
# Replace the all the variables in X by their n × k kernel basis Φ(X)
# equivalent of matrix XX⊤ becomes the n × n kernel matrix K = ΦΦ⊤ with elements kii ′ = φ⊤i φi ′
# k_ij can be directly computed from xi and xi'
#  kii′ = e−γ∥xi −xi′ ∥2 for some γ > 0 (fixe)
n <- length(Airline[,1])
length(Airline[,1])
------------------------------------------------------------------------
# Replace the all the variables in X by their n × k kernel basis Φ(X)
# equivalent of matrix XX⊤ becomes the n × n kernel matrix K = ΦΦ⊤ with elements kii ′ = φ⊤i φi ′
# k_ij can be directly computed from xi and xi'
#  kii′ = e−γ∥xi −xi′ ∥2 for some γ > 0 (fixe)
n <-length(Airline[,1])
------------------------------------------------------------------------
# Replace the all the variables in X by their n × k kernel basis Φ(X)
# equivalent of matrix XX⊤ becomes the n × n kernel matrix K = ΦΦ⊤ with elements kii ′ = φ⊤i φi ′
# k_ij can be directly computed from xi and xi'
#  kii′ = e−γ∥xi −xi′ ∥2 for some γ > 0 (fixe)
n_ <-length(Airline[,1])
------------------------------------------------------------------------
# Replace the all the variables in X by their n × k kernel basis Φ(X)
# equivalent of matrix XX⊤ becomes the n × n kernel matrix K = ΦΦ⊤ with elements kii ′ = φ⊤i φi ′
# k_ij can be directly computed from xi and xi'
#  kii′ = e−γ∥xi −xi′ ∥2 for some γ > 0 (fixe)
n <- 90
K <- matrix(NA,n,n)
------------------------------------------------------------------------
# Replace the all the variables in X by their n × k kernel basis Φ(X)
# equivalent of matrix XX⊤ becomes the n × n kernel matrix K = ΦΦ⊤ with elements kii ′ = φ⊤i φi ′
# k_ij can be directly computed from xi and xi'
#  kii′ = e−γ∥xi −xi′ ∥2 for some γ > 0 (fixe)
n <- 90
K <- matrix(NA,n,n)
K <- matrix(NA,90,90)
plot(Airline)
View(Airline)
plot(Airline$pf,Airline$output)
plot(Airline$If,Airline$output)
plot(Airline$cost,Airline$output)
