---
title: "Supervised Machine Learning HW 4"
author: "Group 3: Maja Nordfeldt, Jakob Rauch, Timo Schenk, Konstantin Sommer"
date: "25-1-2020"
output:
  pdf_document:
      fig_caption: yes
  html_notebook: default
---
# Introduction

# Data
- dropped variable `euribor3m` and `emp.var.rate` because they both contain numerical and date entries.
- make dummies out of categorical variables
- no constant, so we end up with $p=51$ predictors.
- no observations in education illiterate, but model matrix command creates dummy with NANs for it.
- scaled predictors $X$ and outcome $y$ (z-scores).


# Method
The question that we are asking is a classification type question. We are trying to classify based on some available characterisitics if a customer will chose yes or no. We therefore use a method called support vector machine. Its purpose is to predict in a linear regression (can also be extended to nonlinear) type estimation the value of the endogeneous variable, in our case y, if the client subscribed a term deposit. In order to that we are maximizing a loss function, that minimizes the likelihood of being far of the right solution, so if clients with a predicted value that is far from the actual decission is predicted wrong, that influences the loss heavier than closer ones. Additionally we include a penalizing term, that in this case does not only full fill a parameter shrinkage function, but also determines the width of an intervall in which we will not give a prediction about the choice of the client.\\
This loss function that we are aiming to minimize is given by $$L_{Quad-SVM(c;w)}=\sum_{i \in G_{-1}}\max(0; q_i + 1)^2 +\sum_{i \in G_{1}}\max(0;1- q_i)^2+ \lambda w^Tw$$ where $\q_i$ is the predicted value from $c+x_i^Tw$, with $x_i$ the observation of individual $i$, $c$ the constant and $w$ the weights or coefficient vector. $G_{-1}$ and $G_1$ describe the set of individuals for which y is either $-1$ or $1$ so that an observation counts towards the loss function for only of the two terms.\\
The decission rule is then when a predicted value is smaller than $-1$ it will be predicted as $y=-1$ and for $q_i>1$ vice versa. This implies that for all values that are predicted in between they will not be assigned to a yes or no answer. The width of this intervall is determined by the size of the coefficients since they determine the value of $q_i$. This is where the penalty term comes into play, so by chosing different values for $\lambda$ one can determine how much the parameters are shrinked and therefore how large the described interval is. The intuition for the two maxima functions are that an observation either adds zero to the loss function, if it would be predicted correctly or it adds a number that is increasing in the distance to the actual correct value, -1 or 1. Such that values that are only predicted slightly wrong, but will still be classified incorrectly counts less towards the loss than an observation that is preicted far of, but still gets classified the same as the one that is only a bit of.\\
One can instead of minimizing over the squared maximum terms also maximizize over the absolute values or over what is known as a Hinge error term. The advantage of the squared expression is, that there must be a blobal minimum, since the Loss function is strictly convex and that there exists algorithms that can find this minimum in a reasonable time. We are using the so called majoorizing function algorithm for it.\\
In order to find the optimal value of the hyperparameter, we are again relying on the k-fold cross validation. In contrast to other typical regression cases one must now chose a different measure of loss instead of the RMSE. We decided to base our deccission based on the $\lambda$ that minimizes the misclassification rate, which is defined as one minues the sum over all correctly specified observations divided by the number of observations. This loss definition has a straight forawrd intuition since one wants to maximize the number of correctly predicted values.

# Results

# Conclusion

# Code
```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```
