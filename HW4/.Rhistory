for (j in 1:length(lambda_values)){
cat('lambda = ',lambda_values[j])
loss[j] = k_fold_crossval(y,X,k,lambda_values[j],v0) # Fill the matrix with losses
}
min_index = which.min(loss) # Find index of lowest loss
lambda_min = lambda_values[min_index] # lambda value at lowest loss
loss_min = loss[min_index] # lowest misclassification error
return(list(loss,lambda_min,loss_min))
}
### main
# load data
load('./bank.RData')
smp = sample(1:dim(bank)[1],size = 1000,replace = F)
bank = bank[smp,]
y = 2*as.numeric(bank$y)-3   # scale to -1,1 instead of 1,2
# transform categorical variables to dummies
# omit euribor3m, weird entries.
X = model.matrix(y~.-euribor3m -emp.var.rate -1 ,data = bank)
# no observations in education illiterate, but model matrix command creates dummy with NANs for it:
X = scale(X)
idx.missing = which(colSums(is.na(X))>0) # identify index of column containing NAns
X = X[,-idx.missing]
X = cbind(1,X)
# linear SVM
p = ncol(X)-1
# settings
v0 = rep(0,p+1)
lambda_values=seq(0.01, 1, length.out = 20)
epsilon = 1e-8 # the default of the package
# ---- Cross Validation -----------
# manual cross validation of lambda using quadratic hinge loss
man_results=min_loss(y,X,10,lambda_values,v0) # find optimal lambda
loss_vector=man_results[[1]]
opt_lambda=man_results[[2]]
plot(lambda_values,loss_vector,pch=16) # plot average missclassification for different lambdas
# compare cross validation to package
# pack_results=svmmajcrossval(X,y,ngroup=10,search.grid = list(lambda=lambda_values)) ##Find optimal lambda
# cbind(opt_lambda,pack_results$param.opt)
# ----  confusion matrix and hitrate ----
lambda_compare = opt_lambda # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
table(y,ypred)
hitrate = mean(ypred==y)
# ---- density plot ----
qlim = c(min(q),max(q))
cols = c(rgb(1,0,0,0.5),rgb(0,0,1,0.5))
pdf('densityplot.pdf')
hist(q[y==-1],breaks = 50, col =cols[1],
main="Density plot",sub= paste("hitrate = ",hitrate)
,xlim=qlim,xlab='q')
hist(q[y==1],breaks = 30, col =cols[2],add=T)
legend('topright',legend = c('y = -1','y = 1'),
col = cols, lty=1)
box()
dev.off()
# ---- Compare with package for a fixed value of lambda ----
pack_vhat=svmmaj(X[,-1],y,lambda = lambda_compare,hinge="quadratic",scale="none") # get coefficients for optimal beta
print(cbind(vhat[-1],pack_vhat$beta)) # compare our solution to package
print(mean(abs(vhat[-1]-pack_vhat$beta))) # mean absolute deviation of the estimates
sum(ypred!=((pack_vhat$q>0)-(pack_vhat$q<0))) # predictions are the same
# summary(pack_vhat)
plot(pack_results)
summary(pack_results)
lambda_values[2]
lambda_compare = lambda_values[2] # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
table(y,ypred)
hitrate = mean(ypred==y)
pack_vhat=svmmaj(X[,-1],y,lambda = lambda_compare,hinge="quadratic",scale="none") # get coefficients for optimal beta
print(cbind(vhat[-1],pack_vhat$beta)) # compare our solution to package
print(mean(abs(vhat[-1]-pack_vhat$beta))) # mean absolute deviation of the estimates
sum(ypred!=((pack_vhat$q>0)-(pack_vhat$q<0))) # predictions are the s
set.seed(123456)
library(SVMMaj)
### functions
## calculate q g
# q =function(X,c,w) c+X%*%w
## SVM loss
loss.svm.quad= function(y,q,w,lambda){
# returns SVM loss given weights, predicted qs
grp0err = sum(((q+1)[(y==-1)&(q>-1)])^2)
grp1err = sum(((1-q)[(y==1)&(q<1)])^2)
return(grp0err+grp1err+lambda*sum(w^2))
}
## quadratic hinge -- notation as in slides
v.update = function(y,X,vold,lambdaP){
# returns updated v+
qtilde = X%*%vold
b = qtilde*((qtilde<=-1)&(y==-1)) - ((qtilde>-1)&(y==-1)) +
((qtilde<=1)&(y==1)) + qtilde*((qtilde>1)&(y==1))
# A = diag(1,nrow(X)) # skip for computational simplicity
vnew = solve(t(X)%*%X + lambdaP)%*%t(X)%*%b
return(vnew)
}
## SVMMaj MM algorithm
SVMMaj_manual = function(y,X,lambda,v0,epsilon = 1e-8){
# returns optimal v = (c,w)' given data y, X, penalty lambda
# by running the MM algorithm with initial level v0 and stopping criterion
# epsilon
lambdaP = diag(lambda,ncol(X))
lambdaP[1,1] = 0
Lold = loss.svm.quad(y,q = X%*%v0,w = v0[-1],lambda)
#cat('L0 =',Lold,"\n")
k = 1
Lnew = Lold
v = v0
while(k==1|((Lold-Lnew)/Lold)>epsilon) {
k = k+1
Lold = Lnew
vnew = v.update(y,X,v,lambdaP)
Lnew = loss.svm.quad(y,q = X%*%vnew,w = vnew[-1],lambda)
v = vnew
#cat('after iteration ',k," L = ",Lnew,"\n")
}
#cat("stopping crit achieved\n")
return(vnew)
}
### k-fold
k_fold_crossval = function(y,X,k,lambda,v0,reshuffled_indices){
# Function for k-fold crossvalidation, returns RMSE for given alpha and lambda
# and shuffled indices
n = length(y) # Number of observations
p = (dim(X)[2]) # Number of parameters
# Reshuffle data and split into k groups of equal size
reshuffled_indices = sample(1:n, n, replace=FALSE) # Shuffle indices of our n observations
n_test = floor(n/k) # n=77 and k=10 would give 7 obs. in the test set the rest of the observations
# in the training set. For n=77, using k=11 or k=7 is advisable to get evenly sized groups.
loss=rep(0,k) # Vector to hold the loss for each fold
# Loop over the k folds and save MSEs
for (i in 1:k){
# Divide data into test and training
y_test = y[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
y_train = y[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
X_test = X[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
X_train = X[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
beta = SVMMaj_manual(y_train,X_train,lambda,v0) # get beta estimates for training data from majorization algorithm
fitted_val = (X_test)%*%beta # get qhat for test data
TP=sum(fitted_val>0&y_test==1)
TN=sum(fitted_val<0&y_test==(-1))
loss[i]=1-(TP+TN)/n_test # save average missclassification for test data
cat('.')
}
loss=mean(loss) ##take average over found losses
cat('mean loss for lambda =',lambda,':',loss)
cat('\n')
return(loss)
}
min_loss = function(y,X,k=10,lambda_values=10^seq(-3, 4, length.out = 10),v0){
# Tunes hyperparameters lambda via k-fold crossvalidation. Returns optimal lambda
# as well as the resulting loss
loss = array(0,length(lambda_values)) # create vector to hold loss for each lambda
# Loop over hyperparameter combinations
for (j in 1:length(lambda_values)){
# cat('lambda = ',lambda_values[j])
loss[j] = k_fold_crossval(y,X,k,lambda_values[j],v0) # Fill the matrix with losses
}
min_index = which.min(loss) # Find index of lowest loss
lambda_min = lambda_values[min_index] # lambda value at lowest loss
loss_min = loss[min_index] # lowest misclassification error
return(list(loss,lambda_min,loss_min))
}
### main
# load data
load('./bank.RData')
smp = sample(1:dim(bank)[1],size = 1000,replace = F)
bank = bank[smp,]
y = 2*as.numeric(bank$y)-3   # scale to -1,1 instead of 1,2
# transform categorical variables to dummies
# omit euribor3m, weird entries.
X = model.matrix(y~.-euribor3m -emp.var.rate -1 ,data = bank)
# no observations in education illiterate, but model matrix command creates dummy with NANs for it:
X = scale(X)
idx.missing = which(colSums(is.na(X))>0) # identify index of column containing NAns
X = X[,-idx.missing]
X = cbind(1,X)
# linear SVM
p = ncol(X)-1
# settings
v0 = rep(0,p+1)
lambda_values=seq(0.01, 1, length.out = 20)
epsilon = 1e-8 # the default of the package
# ---- Cross Validation -----------
# manual cross validation of lambda using quadratic hinge loss
man_results=min_loss(y,X,10,lambda_values,v0) # find optimal lambda
loss_vector=man_results[[1]]
opt_lambda=man_results[[2]]
plot(lambda_values,loss_vector,pch=16) # plot average missclassification for different lambdas
# compare cross validation to package
# pack_results=svmmajcrossval(X,y,ngroup=10,search.grid = list(lambda=lambda_values)) ##Find optimal lambda
# cbind(opt_lambda,pack_results$param.opt)
# ----  confusion matrix and hitrate ----
lambda_compare = lambda_values[2] # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
table(y,ypred)
hitrate = mean(ypred==y)
# ---- density plot ----
qlim = c(min(q),max(q))
cols = c(rgb(1,0,0,0.5),rgb(0,0,1,0.5))
pdf('densityplot.pdf')
hist(q[y==-1],breaks = 50, col =cols[1],
main="Density plot",sub= paste("hitrate = ",hitrate)
,xlim=qlim,xlab='q')
hist(q[y==1],breaks = 30, col =cols[2],add=T)
legend('topright',legend = c('y = -1','y = 1'),
col = cols, lty=1)
box()
dev.off()
# ---- Compare with package for a fixed value of lambda ----
pack_vhat=svmmaj(X[,-1],y,lambda = lambda_compare,hinge="quadratic",scale="none") # get coefficients for optimal beta
print(cbind(vhat[-1],pack_vhat$beta)) # compare our solution to package
print(mean(abs(vhat[-1]-pack_vhat$beta))) # mean absolute deviation of the estimates
sum(ypred!=((pack_vhat$q>0)-(pack_vhat$q<0))) # predictions are the same
# summary(pack_vhat)
plot(lambda_values,loss_vector,pch=16) # plot average missclassification for different lambdas
plot(lambda_values,loss_vector,type='l',
ylim=c(0,0.15),xlab='lambda',ylab='missclassification rate',
main='Cross-validation performance per grid value') # plot average missclassification for different lambdas
plot(pack_results)
cbind(loss_vector,pack_results$loss.grp)
summary(pack_results)
pack_results$loss.opt
pack_results$loss.grp
smry.cval.pkg = summary(pack_results)
pack_results
pack_results$param.grid$loss
rev(loss_vector)
pdf('cvalplot.pdf')
plot(lambda_values,loss_vector,type='l',
ylim=c(0,0.15),xlab='lambda',ylab='missclassification rate',
main='Cross-validation performance per grid value') # plot average missclassification for different lambdas
dev.off()
table(bank$y,bank$duration)
hist(bank$duration[bank$y==1],col=rgb(1,0,0,0.5))
hist(bank$duration[bank$y==1],breaks=30,col=rgb(1,0,0,0.5))
hist(bank$duration[bank$y==1,breaks=30,col=rgb(1,0,0,0.5))
hist(bank$duration,breaks=30,col=rgb(1,0,0,0.5))
hist(bank$duration[bank$y==1],breaks=30,col=rgb(1,0,0,0.5))
bank$y==1
sum(bank$y==1)
table(bank$y)
sum(bank$y=='no')
hist(bank$duration[bank$y=='no'],breaks=30,col=rgb(1,0,0,0.5))
hist(bank$duration[bank$y=='yes'],breaks=30,col=rgb(0,0,1,0.5),add=T)
set.seed(123456)
library(SVMMaj)
### functions
## calculate q g
# q =function(X,c,w) c+X%*%w
## SVM loss
loss.svm.quad= function(y,q,w,lambda){
# returns SVM loss given weights, predicted qs
grp0err = sum(((q+1)[(y==-1)&(q>-1)])^2)
grp1err = sum(((1-q)[(y==1)&(q<1)])^2)
return(grp0err+grp1err+lambda*sum(w^2))
}
## quadratic hinge -- notation as in slides
v.update = function(y,X,vold,lambdaP){
# returns updated v+
qtilde = X%*%vold
b = qtilde*((qtilde<=-1)&(y==-1)) - ((qtilde>-1)&(y==-1)) +
((qtilde<=1)&(y==1)) + qtilde*((qtilde>1)&(y==1))
# A = diag(1,nrow(X)) # skip for computational simplicity
vnew = solve(t(X)%*%X + lambdaP)%*%t(X)%*%b
return(vnew)
}
## SVMMaj MM algorithm
SVMMaj_manual = function(y,X,lambda,v0,epsilon = 1e-8){
# returns optimal v = (c,w)' given data y, X, penalty lambda
# by running the MM algorithm with initial level v0 and stopping criterion
# epsilon
lambdaP = diag(lambda,ncol(X))
lambdaP[1,1] = 0
Lold = loss.svm.quad(y,q = X%*%v0,w = v0[-1],lambda)
#cat('L0 =',Lold,"\n")
k = 1
Lnew = Lold
v = v0
while(k==1|((Lold-Lnew)/Lold)>epsilon) {
k = k+1
Lold = Lnew
vnew = v.update(y,X,v,lambdaP)
Lnew = loss.svm.quad(y,q = X%*%vnew,w = vnew[-1],lambda)
v = vnew
#cat('after iteration ',k," L = ",Lnew,"\n")
}
#cat("stopping crit achieved\n")
return(vnew)
}
### k-fold
k_fold_crossval = function(y,X,k,lambda,v0,reshuffled_indices){
# Function for k-fold crossvalidation, returns RMSE for given alpha and lambda
# and shuffled indices
n = length(y) # Number of observations
p = (dim(X)[2]) # Number of parameters
# Reshuffle data and split into k groups of equal size
reshuffled_indices = sample(1:n, n, replace=FALSE) # Shuffle indices of our n observations
n_test = floor(n/k) # n=77 and k=10 would give 7 obs. in the test set the rest of the observations
# in the training set. For n=77, using k=11 or k=7 is advisable to get evenly sized groups.
loss=rep(0,k) # Vector to hold the loss for each fold
# Loop over the k folds and save MSEs
for (i in 1:k){
# Divide data into test and training
y_test = y[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
y_train = y[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)])]
X_test = X[c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
X_train = X[-c(reshuffled_indices[(1+(i-1)*n_test):(i*n_test)]),]
beta = SVMMaj_manual(y_train,X_train,lambda,v0) # get beta estimates for training data from majorization algorithm
fitted_val = (X_test)%*%beta # get qhat for test data
TP=sum(fitted_val>0&y_test==1)
TN=sum(fitted_val<0&y_test==(-1))
loss[i]=1-(TP+TN)/n_test # save average missclassification for test data
cat('.')
}
loss=mean(loss) ##take average over found losses
cat('mean loss for lambda =',lambda,':',loss)
cat('\n')
return(loss)
}
min_loss = function(y,X,k=10,lambda_values=10^seq(-3, 4, length.out = 10),v0){
# Tunes hyperparameters lambda via k-fold crossvalidation. Returns optimal lambda
# as well as the resulting loss
loss = array(0,length(lambda_values)) # create vector to hold loss for each lambda
# Loop over hyperparameter combinations
for (j in 1:length(lambda_values)){
# cat('lambda = ',lambda_values[j])
loss[j] = k_fold_crossval(y,X,k,lambda_values[j],v0) # Fill the matrix with losses
}
min_index = which.min(loss) # Find index of lowest loss
lambda_min = lambda_values[min_index] # lambda value at lowest loss
loss_min = loss[min_index] # lowest misclassification error
return(list(loss,lambda_min,loss_min))
}
### main
# load data
load('./bank.RData')
smp = sample(1:dim(bank)[1],size = 1000,replace = F)
bank = bank[smp,]
y = 2*as.numeric(bank$y)-3   # scale to -1,1 instead of 1,2
# transform categorical variables to dummies
# omit euribor3m, weird entries.
X = model.matrix(y~.-euribor3m -emp.var.rate -duration -1 ,data = bank)
# no observations in education illiterate, but model matrix command creates dummy with NANs for it:
X = scale(X)
idx.missing = which(colSums(is.na(X))>0) # identify index of column containing NAns
X = X[,-idx.missing]
X = cbind(1,X)
# linear SVM
p = ncol(X)-1
# settings
v0 = rep(0,p+1)
lambda_values=seq(0.01, 1, length.out = 20)
epsilon = 1e-8 # the default of the package
# ---- Cross Validation -----------
# manual cross validation of lambda using quadratic hinge loss
man_results=min_loss(y,X,10,lambda_values,v0) # find optimal lambda
loss_vector=man_results[[1]]
opt_lambda=man_results[[2]]
pdf('cvalplot.pdf')
plot(lambda_values,loss_vector,type='l',
ylim=c(0,0.15),xlab='lambda',ylab='missclassification rate',
main='Cross-validation performance per grid value') # plot average missclassification for different lambdas
dev.off()
# compare cross validation to package
# pack_results=svmmajcrossval(X,y,ngroup=10,search.grid = list(lambda=lambda_values)) ##Find optimal lambda
# cbind(opt_lambda,pack_results$param.opt)
# ----  confusion matrix and hitrate ----
lambda_compare = lambda_values[2] # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
table(y,ypred)
hitrate = mean(ypred==y)
# ---- density plot ----
qlim = c(min(q),max(q))
cols = c(rgb(1,0,0,0.5),rgb(0,0,1,0.5))
pdf('densityplot.pdf')
hist(q[y==-1],breaks = 50, col =cols[1],
main="Density plot",sub= paste("hitrate = ",hitrate)
,xlim=qlim,xlab='q')
hist(q[y==1],breaks = 30, col =cols[2],add=T)
legend('topright',legend = c('y = -1','y = 1'),
col = cols, lty=1)
box()
dev.off()
# ---- Compare with package for a fixed value of lambda ----
pack_vhat=svmmaj(X[,-1],y,lambda = lambda_compare,hinge="quadratic",scale="none") # get coefficients for optimal beta
print(cbind(vhat[-1],pack_vhat$beta)) # compare our solution to package
print(mean(abs(vhat[-1]-pack_vhat$beta))) # mean absolute deviation of the estimates
sum(ypred!=((pack_vhat$q>0)-(pack_vhat$q<0))) # predictions are the same
# summary(pack_vhat)
plot(lambda_values,loss_vector,type='l',
ylim=c(0,0.15),xlab='lambda',ylab='missclassification rate',
main='Cross-validation performance per grid value') # plot average missclassification for di
pack_results=svmmajcrossval(X,y,ngroup=10,search.grid = list(lambda=lambda_values)) ##Find optimal lambda
cbind(opt_lambda,pack_results$param.opt)
lambda_compare = opt_lambda # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
table(y,ypred)
hitrate = mean(ypred==y)
hist(q[y==-1],breaks = 50, col =cols[1],
main="Density plot",sub= paste("hitrate = ",hitrate)
,xlim=qlim,xlab='q')
hist(q[y==1],breaks = 30, col =cols[2],add=T)
legend('topright',legend = c('y = -1','y = 1'),
col = cols, lty=1)
hist(q[y==-1],breaks = 50, col =cols[1],
main="Density plot",
sub= paste("hitrate = ",hitrate,'lambda =',lambda_compare),
xlim=qlim,xlab='q')
hist(q[y==1],breaks = 30, col =cols[2],add=T)
legend('topright',legend = c('y = -1','y = 1'),
col = cols, lty=1)
box()
hist(q[y==-1],breaks = 50, col =cols[1],
main="Density plot",
sub= paste("hitrate = ",hitrate,'lambda =',round(lambda_compare,3)),
xlim=qlim,xlab='q')
hist(q[y==1],breaks = 30, col =cols[2],add=T)
legend('topright',legend = c('y = -1','y = 1'),
col = cols, lty=1)
plot(pack_results)
pdf('densityplot.pdf')
hist(q[y==-1],breaks = 50, col =cols[1],
main="Density plot",
sub= paste("hitrate = ",hitrate,', lambda =',round(lambda_compare,3)),
xlim=qlim,xlab='q')
hist(q[y==1],breaks = 30, col =cols[2],add=T)
legend('topright',legend = c('y = -1','y = 1'),
col = cols, lty=1)
box()
dev.off()
pdf('cvalplot.pdf')
plot(lambda_values,loss_vector,type='l',
ylim=c(0,0.15),xlab='lambda',ylab='missclassification rate',
main='Cross-validation performance per grid value') # plot average missclassification for different lambdas
dev.off()
min(loss_vector)
# ----  confusion matrix and hitrate ----
lambda_compare = 0.427 # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
order(abs(vhat))[1:5] # largest 5 coefficients
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
table(y,ypred)
hitrate = mean(ypred==y)
vhat
order(abs(vhat)) # largest 5 coefficients
abs(vhat)
vhat[rank(abs(vhat))<=5] # largest 5 coefficients
vhat[rank(abs(vhat))<=5,] # largest 5 coefficients
vhat[order(abs(vhat))<=5,] # largest 5 coefficients
rank(abs(vhat))
vhat[rank(abs(vhat))==1,] # largest 5 coefficients
vhat[rank(-abs(vhat))==1,] # largest 5 coefficients
vhat[rank(-abs(vhat))<=5,] # largest 5 coefficients
(vhat[rank(-abs(vhat))<=6,])[-1] # largest 5 coefficients except constant
kable((vhat[rank(-abs(vhat))<=6,])[-1]) # largest 5 coefficients except constant
knitr::kable((vhat[rank(-abs(vhat))<=6,])[-1]) # largest 5 coefficients except constant
pander((vhat[rank(-abs(vhat))<=6,])[-1]) # largest 5 coefficients except constant
library("pander") # for tables
install.packages("pander")
library("pander") # for tables
pander((vhat[rank(-abs(vhat))<=6,])[-1]) # largest 5 coefficients except constant
# ----  confusion matrix and hitrate ----
lambda_compare = 0.427 # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
pander((vhat[rank(-abs(vhat))<=6,])[-1],
caption='largest 5 coefficients') # largest 5 coefficients except constant
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
table(y,ypred)
hitrate = mean(ypred==y)
vhat
library(summarytools)
install.packages("summarytools")
# ----  confusion matrix and hitrate ----
lambda_compare = 0.427 # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
pander((vhat[rank(-abs(vhat))<=6,])[-1],
caption='largest 5 coefficients') # largest 5 coefficients except constant
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
tb = table(y,ypred)
pander(as.matrix(tb))
hitrate = mean(ypred==y)
# ----  confusion matrix and hitrate ----
lambda_compare = 0.427 # or set to opt_lambda from CV
vhat=SVMMaj_manual(y,X,lambda_compare,v0,epsilon) # manual estimates
pander((vhat[rank(-abs(vhat))<=6,])[-1],
caption='largest 5 coefficients') # largest 5 coefficients except constant
q = X%*%vhat
ypred = 1*(q>0) -1*(q<0)
table(y,ypred)
hitrate = mean(ypred==y)
install.packages("kernlab")
install.packages("kernlab")
# Results different Kernels
library(kernlab)
pack_vhat_rbf <- svmmaj(X,y,lambda = lambda_compare,hinge="quadratic",scale="none", kernel=rbfdot, kernel.sigma=1)  # RBF kernel, sigma = 1
library(SVMMaj)
# Results different Kernels
library(kernlab)
pack_vhat_rbf <- svmmaj(X,y,lambda = lambda_compare,hinge="quadratic",scale="none", kernel=rbfdot, kernel.sigma=1)  # RBF kernel, sigma = 1
pack_vhat_poly <- svmmaj(X,y,lambda = lambda_compare,hinge="quadratic",scale="none", kernel=polydot,kernel.degree=2,kernel.offset =1,kernel.scale=1) # Poly kernel degree 2, added term = 1
