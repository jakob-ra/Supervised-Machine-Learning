---
title: "Supervised Machine Learning HW 3"
author: "Group 3: Maja Nordfeldt, Jakob Rauch, Timo Schenk, Konstantin Sommer"
date: "25-1-2020"
output:
  pdf_document:
      fig_caption: yes
  html_notebook: default
---

# Introduction
Can we predict an airline's revenue passenger miles (RPM) in a given year using fuel price and airline characteristics? RPM indicates how many miles an airline transports its paying customers and is a widely used output measure in the aviation industry. For airlines' financial and stratgic planning, the prediction problem is therefore of obvious importance. 

However, the problem presents a number of practical challenges: First, we only have a very limited set of predictors available. To complicate things, there might be two-way or even three-way interactions between the predictor variables that significantly affect output. Finally, there might be important nonlinearities in the relation between our predictors and output: For instance, when fuel prices rise above a certain threshold the airline might find that servicing some market segments has become entirely unprofitable and therefore reduce its operations. Then the effect of fuel price on RPM is limited for low fuel prices but large for already high fuel prices.

To deal with these problems, we use the method of kernel ridge regression (KRR). In the following, we will give an overview of our data, explain our method and modeling choices in detail, and discuss our results.

# Data 

We have a balanced panel of six airlines spanning the years 1970 to 1984. The data was originally used by Greene et al. (1997) and is supplied with the R package Ecdat. The panel contains 4 variables: fuel price, total costs of the airline in 1,000$, load factor (the average capacity utilization of the fleet) and output in RPM (as already explained). All of these variables are specific to a given airline. This seems to be true even for the variable fuel price, perhaps indicating that airlines get different fuel prices as a result of quantity discounts. We are only speculating,however, as we do not have a detailed description of this variable. 

Table 1 gives an overview of the differences between airlines. As we can see, the airlines vary wildly in size, with airline 1 having more than ten times the output of airline 2. All airlines do seem to pay roughly the same price for fuel, although the bigger airlines seem to benefit from a small quantity discount. Bigger airlines also seem to do better in terms of load, i.e. utilizing their fleet to the fullest. This relation, however, does not hold in every case.

```{r,echo=F, message=F}
# Clear cache
rm(list = ls())

# Load packages
library(dsmle)
library(plyr)
library(pander)

# Load supermarket data from github
githubURL = "https://github.com/jakob-ra/Supervised-Machine-Learning/raw/master/HW3/Airline.RData"
load(url(githubURL))
attach(Airline)

# Descriptive statistics
pander(ddply(Airline, .(airline), summarize,  Cost=mean(cost), Output=mean(output), Fuel_Price=mean(pf), Load_Factor=mean(lf)),
       caption = "Airline averages over all years")
```

Table 2 gives an overview of the time trends across all airlines. Clearly, output has grown a lot over the 15 years, with only a short interruption around 1980. Costs have grown even stronger, although this might be due to inflation Again, we do not know for lack of a detailed variable description, but if the costs are really not inflation adjusted this is a limitation of the data. Fuel prices, on the other hand, have fallen a lot over the years, indicating perhaps the recovery from the oil crisis. Finally, the airlines seem to have improved their efficiency over the years and have a 10% higher load factor at the end of the sample period.

```{r,echo=F, message=F}
pander(ddply(Airline, .(year), summarize,  Cost=mean(cost), Output=mean(output), Fuel_Price=mean(pf), Load_Factor=mean(lf)),
       caption = "Yearly averages over all airlines")
```

Note that from the original dataset we transform all variables to z-scores, removing their unit of measure. After this transformation, each variables value can be interpreted as standard deviations from its mean. The importance of this will be explained in the next section. Additionally we will transform the panel data set to a cross sectional one, by transforming all categorical variables, airline and year, into dummys. Such that we have one dummy per airline and one per year. Their coefficients could in an OLS context be interpreted as fixed effects.




# Methods
We employ two types of *kernel ridge regression* for our predictions. In general, a kernel ridge regression combines the penalty of ridge regression to avoid over-fitting, with a set of non-linear prediction functions in a computationally simplified way to allow for model complexity. This means the method handles issues tied to the trade-off between model complexity and variance well. For example, when opting for a more complex, non-linear model over a simpler option, one is often still limited in what complexity level that is feasible and can produce adequate forecast precision due to the uncertainty in model choice and parameter estimation. Meanwhile, a high number of predictors can provide a better fit, but risk to worsens predictions if observations are sparse, why a reduction is desirable. Kernel ridge regression handles both of theses concerns, and is particularly attractive for estimating non-linear models of “fat data”, where the number of predictors greatly exceed the number of observations. 

With kernel ridge regression, we map predictors to a high-dimensional space of nonlinear functions, and in this space the forecast is estimated, with the penalty term counteracting overfitting. However, by setting the kernel in a convenient way, calculations need not to be done in the high-dimensional space, but in a simpler manner as will be explained later. 

For each observation $i$, we want to predict $y_i$ with $\mathbf{x_i}$, and seek a function of  $f(\mathbf{x_i})$ which approximates $y_i$ well for all our observations. We will choose $f(\mathbf{x})$  from a set of linear combinations of the elements of $\phi(\mathbf{x})$, a function mapping the data from the input data dimension space P to one of higher dimensions. With a large number of regressors after the mapping, the functional form will be very flexible, but computationally effortsome. With a kernel function $K()$, where $K(\mathbf{x_i},\mathbf{x_j}) = \phi(\mathbf{x_i})^T\phi(\mathbf{x_j})$, representing a dot product in the higher dimensional space, we can find the dot product under P-dimensional computations. This means we avoid explicitly transforming each data point with $\mathbf{\phi}$, but can instead represent them with pairwise values - the kernel values of pairs of points. In other words, instead of mapping points into the higher dimensional space, the data is represented by the kernel matrix **K**, over which all the computations can be done. The effective dimensions of the feature space is also much lower than the one introduced by the kernel, as all basis functions are shrinked.Kernels can also be thought of as functions measuring how closely related the input vectors are - if they are similar the kernel output will be large, and dissimilar it will be small. 

Decisions of the method include choosing the kernel used, as well as the value of the hyper parameters. The kernel choice will determine the functional form, and setting tuning parameters can require a trade-off between effortful cross-validation or non-optimal results.  Here we have chosen two common kernels and used cross-validation for our hyperparameters.

The first kernel we use is the *radial basis function kernel*, for which we have $k_{ij} = exp(\gamma||\mathbf{x_i} - \mathbf{x_j}||^2)$, with $\gamma > 0$. If $\gamma = 2\sigma^{-1}$, it is referred to as the *gaussian kernel*. Generally, this kernel corresponds to mapping of data to an infinite dimensional space. The $\gamma$ parameter sets the spread of the RBF kernel. A larger $\gamma$  implies more local kernel functions, increasing the effective dimensions and capturing more complexity and shape of the data. 

The second kernel we employ is the *inhomogeneous polynomial kernel*, under which we can do a polynomial regression with high-order polynomials in a very short amount of time. Here we have $k_{ij} = (1 + \mathbf{x_i}^T\mathbf{x_j})^d$, for a degree $d > 0$. The degree determines the highest polynomial degree assigned to a predictor, and translates into how many basis functions that are created - $d+p\choose d$ - each basis function representing a the predictors at polynomial degree up to d and their interactions. In the higher dimensional space, we can use linear methods, while exploring non-linear patterns and relationships as the terms are non-linear transformations of the original data. 

As in the case of a ridge regression it is essential to standardize our variables to z-scores such that they will be penalized on an equal basis, which would not be the case if the variables would exhibit different scales and variances. As already stated in the introduction we assume that their might be interactions between our variables that can help us predicting our endogenous variable. For example we assume that there might be synergy effects between the fuel price of a company and the load factor in the sense that cheap fuel might in combination with a high load factor further improve the airline outcome on top of the two coefficients themselves. Additionally we include second degree polynomials for all variables, allowing for a richer spectrum of predictors that are able to capture nonlinear effects.

This leads us to the situation in which our number of predictors exceeds our number of observations. Luckily the above described Kernel estimations can handle such situations. By using the so called dual approach, we will not be minimizing over our actual coefficients but over $q$, which is defined as the predicted values $Xw$, where $w$ are the regression weights. We are using the analytical formula provided in the slides and replace the Ridge Kernel by the above described ones such that: $(\tilde q)=(I+\lambda K^{-1})^{-1}y$, where we oppress the $J$ matrix since our variables are already standardized and therefore demeaned. The penalty term $\lambda$ will be determined i a k-fold cross validation together with either $\gamma$ or $d$. For this method, we split our data sets into training and test data and use our training data to estimate $(\tilde q)$ with different choices for the hyperparameters. We then try to predict the outcome of the test data set and estimate the root mean squared error (RMSE) for every choice of the hyperparameters. For the prediction we again use the formula provided in the slides (slides 57/58). We then vary test and trainings data set and choose in the end the hyperparameters that minimize "on average" the RMSE.





# Results


```{r,include=FALSE, fig.cap="\\label{fig:figs}Figure caption"}
```


# Conclusion & Discussion


# References
Greene, William. "Frontier production functions, M. Hashem Pesaran and Peter Schmidt (eds.): Handbook of Applied Econometrics, vol. II." (1997): 81-166.

# Code
```{r ref.label=knitr::all_labels(), echo = T, eval = F}
```
